{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import operator\n",
    "import sys\n",
    "import math\n",
    "from collections import deque\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation\n",
    "from keras.layers.core import Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.metrics import mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create DQN agent class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size, mode=\"Normal\"): #mode: Normal DQN or Double DQN\n",
    "        '''\n",
    "        parameters setting for DQN\n",
    "        arg: state_size: environment states \n",
    "        arg: action_size: environment action set\n",
    "        arg: mode=(default=Normal|Double): Normal DQN or Double DQN\n",
    "        '''\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.train_start = 400\n",
    "        self.gamma = 0.99 #reward discount rate\n",
    "        self.epsilon = 1.0 #exploration rate\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.learning_rate = 0.0001\n",
    "        self.policy_model = self._build_model() \n",
    "        self.target_model = self._build_model() #Double DQN for stable trainning\n",
    "        self.mode = mode\n",
    "\n",
    "    def _build_model(self):\n",
    "        '''\n",
    "        network declaration\n",
    "        '''\n",
    "        model = Sequential()\n",
    "        model.add(Dense(24, input_dim = self.state_size, kernel_initializer='random_uniform'))\n",
    "        model.add(Dense(24, activation = 'relu'))\n",
    "        model.add(Dense(self.action_size, activation = 'linear'))\n",
    "        model.compile(loss = 'mse', optimizer = Adam(lr = self.learning_rate, beta_1=0.9, beta_2=0.999)) \n",
    "        return model\n",
    "\n",
    "    def act(self, state, explore=True):\n",
    "        '''\n",
    "        to explore new action, or exploit the action that lead to maximum value\n",
    "        arg: state\n",
    "        arg: expolre: explore or exploit mode, default set explore mode\n",
    "        return: action that leads to the maximum predicted value\n",
    "        '''\n",
    "        # explore\n",
    "        if explore and np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "       \n",
    "        #exploit\n",
    "        act_values = self.policy_model.predict(state)\n",
    "        return np.argmax(act_values)\n",
    "\n",
    "    def replay(self, samples, weights): \n",
    "        '''\n",
    "        experience reply for trainning agent, loss measured by td-error\n",
    "        arg: samples: batch of sampled experiences \n",
    "        arg: weights: importance sampling weights\n",
    "        return: td-error to update experience's priority\n",
    "        '''\n",
    "        x_batch = []\n",
    "        y_batch = []\n",
    "        td_error = []\n",
    "        for state, action, reward, next_state, done in samples:            \n",
    "            #calculate the target Q values of each state\n",
    "            current_q = self.policy_model.predict(state)\n",
    "            if done == True:\n",
    "                target_q = reward             \n",
    "            else:\n",
    "                if self.mode == \"Double\":\n",
    "                    #poliy model: action selection\n",
    "                    action = np.argmax(self.policy_model.predict(next_state))\n",
    "                    #target model: action evaluation\n",
    "                    future_reward = self.target_model.predict(next_state)[0][action]\n",
    "                    target_q = reward + self.gamma * future_reward\n",
    "                elif self.mode == \"Normal\":\n",
    "                    #normal DQN\n",
    "                    target_q = reward + self.gamma * np.amax(self.policy_model.predict(next_state))\n",
    "            \n",
    "            #calculate td_error for updating priority\n",
    "            td_error.append(abs(current_q[0][action] - target_q))\n",
    "            \n",
    "            #replace current_q with target_q \n",
    "            current_q[0][action] = target_q \n",
    "            \n",
    "            x_batch.append(state[0])\n",
    "            y_batch.append(current_q[0])\n",
    "        \n",
    "        #train model to get closer to target q values\n",
    "        self.policy_model.fit(np.array(x_batch), np.array(y_batch), epochs = 1, sample_weight = weights, verbose = 0)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "            \n",
    "        return td_error\n",
    "\n",
    " \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Heap Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryHeap(object):\n",
    "    def __init__(self, buffer_size, replace=True):\n",
    "        '''\n",
    "        priority queue as a binary heap structure\n",
    "        arg: buffer_size:  memory buffer size\n",
    "        arg: replace: default True\n",
    "        '''\n",
    "        self.max_size = buffer_size\n",
    "        self.queue_size = 0\n",
    "        self.priority_queue = {}  #{1: (td-error, e_id),.. }\n",
    "        self.rank_to_experience = {}\n",
    "        self.replace = replace\n",
    "    \n",
    "    def isFull(self):\n",
    "        if self.queue_size > self.max_size:\n",
    "            return True\n",
    "        \n",
    "    def get_max_priority(self):\n",
    "        '''\n",
    "        new experience always gets prioritized to ensure unseen event is replayed\n",
    "        return: 1\n",
    "        '''\n",
    "        if self.queue_size == 0: #no entry before\n",
    "            return 1\n",
    "        else:\n",
    "            return self.priority_queue[1][0] #always get the top priority\n",
    "        \n",
    "    def add(self, priority, e_id):\n",
    "        '''\n",
    "        add new experience to priority queue\n",
    "        arg: priority: td-error as priority\n",
    "        arg: e_id: experience id\n",
    "        return: True\n",
    "        '''\n",
    "        self.queue_size += 1\n",
    "        if self.isFull() and self.replace == False:\n",
    "            sys.exit('Error: priority queue is full and replace is set to FALSE!\\n')\n",
    "            return False\n",
    "        self.tmp_rank = min(self.queue_size, self.max_size)\n",
    "        \n",
    "        self.priority_queue[self.tmp_rank] = (priority, e_id)\n",
    "        self.rank_to_experience[self.tmp_rank] = e_id\n",
    "        self.up_heap(self.tmp_rank)\n",
    "        return True\n",
    "        \n",
    "    def update(self, priority, e_id): \n",
    "        '''\n",
    "        update old experience with its new priority, or do insert for new experience\n",
    "        arg: priority: td-error as priority\n",
    "        arg: e_id: experience id\n",
    "        '''\n",
    "        if e_id in self.rank_to_experience.values(): #old experience, do update\n",
    "            inv = {v: k for k, v in self.rank_to_experience.items()}\n",
    "            rank_id = inv[e_id]\n",
    "            self.priority_queue[rank_id] = (priority, e_id)\n",
    "            self.rank_to_experience[rank_id] = e_id\n",
    "            \n",
    "            self.down_heap(rank_id)\n",
    "            self.up_heap(rank_id)\n",
    "            return True     \n",
    "        else: #new experience, do insert\n",
    "            return self.add(priority, e_id)\n",
    "            \n",
    "    \n",
    "    def up_heap(self, node):\n",
    "        '''\n",
    "        upheap balance\n",
    "        arg: node: current rank node\n",
    "        '''\n",
    "        if node > 1:\n",
    "            parent = node // 2\n",
    "            if self.priority_queue[node][0] >= self.priority_queue[parent][0]:\n",
    "                tmp = self.priority_queue[parent]\n",
    "                self.priority_queue[parent] = self.priority_queue[node]\n",
    "                self.priority_queue[node] = tmp\n",
    "                #change rank_to_experience\n",
    "                self.rank_to_experience[parent] = self.priority_queue[parent][1]\n",
    "                self.rank_to_experience[node] = self.priority_queue[node][1]\n",
    "                self.up_heap(parent)\n",
    "        \n",
    "    def down_heap(self, node):\n",
    "        '''\n",
    "        downheap balance\n",
    "        arg: node: current rank node\n",
    "        '''\n",
    "        if node < self.queue_size:\n",
    "            biggest = node\n",
    "            left = node * 2\n",
    "            right = node * 2 + 1\n",
    "            if left < self.queue_size and self.priority_queue[node][0] < self.priority_queue[left][0]:\n",
    "                biggest = left\n",
    "            if right < self.queue_size and self.priority_queue[node][0] < self.priority_queue[right][0]:\n",
    "                biggest = right\n",
    "        \n",
    "            if biggest != node:\n",
    "                tmp = self.priority_queue[biggest]\n",
    "                self.priority_queue[biggest] = self.priority_queue[node]\n",
    "                self.priority_queue[node] = tmp\n",
    "                #change rank_to_experience\n",
    "                self.rank_to_experience[biggest] = self.priority_queue[biggest][1]\n",
    "                self.rank_to_experience[node] = self.priority_queue[node][1]\n",
    "                self.down_heap(biggest)\n",
    "\n",
    "    def rebalance(self, full=False):\n",
    "        '''\n",
    "        sort binary heap \n",
    "        '''\n",
    "        if full:\n",
    "            sorted_list = sorted(self.priority_queue.values(), key=lambda x: x[0], reverse=True)\n",
    "            self.priority_queue.clear()\n",
    "            self.rank_to_experience.clear()\n",
    "            rank = 1\n",
    "            while rank <= self.queue_size:\n",
    "                self.priority_queue[rank] = sorted_list[rank-1]\n",
    "                self.rank_to_experience[rank] = sorted_list[rank-1][1]\n",
    "                rank += 1\n",
    "            for i in range(int(self.queue_size // 2), 1, -1):\n",
    "                self.down_heap(i)\n",
    "    \n",
    "    def get_experience_id(self, rank_ids):\n",
    "        '''\n",
    "        retrieve experience ids \n",
    "        arg: priority_ids: list of rank ids\n",
    "        return: experience ids\n",
    "        '''\n",
    "        return [self.rank_to_experience[i] for i in rank_ids]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rank Proritized Replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RankBuffer:\n",
    "    '''\n",
    "    memory buffer for rank-based prioritized experience replay \n",
    "    '''\n",
    "    def __init__(self, minibatch):\n",
    "        self.total_steps = 100000\n",
    "        self.train_start= 400    \n",
    "        self.memory = {}\n",
    "        self.memory_size = 20000\n",
    "        self.record_size = 0\n",
    "        self.index = 0\n",
    "        self.priority_queue = BinaryHeap(self.memory_size)\n",
    "        self.isFull = False\n",
    "        self.replace = True\n",
    "        \n",
    "        self.alpha = 0.7 #alpha: 1 to 0 (how much a transition is to be reused)\n",
    "        self.beta= 0.7 #beta: 0 to 1 (how much importance to give for prioritized experience)\n",
    "        self.k = minibatch\n",
    "        self.n_partitions = 400 # partition number N, split total size to N part\n",
    "            \n",
    "        self.distributions = self.build_distributions()\n",
    "        self.beta_grad = (1 - self.beta) / (self.total_steps - self.train_start)\n",
    "    \n",
    "    def get_index(self):\n",
    "        '''\n",
    "        get index for new experience in the memory buffer\n",
    "        return: index \n",
    "        '''\n",
    "        if self.record_size <= self.memory_size:\n",
    "            self.record_size += 1\n",
    "        \n",
    "        if len(self.memory) == self.memory_size:\n",
    "            self.isFull = True\n",
    "            #when memory is full, reconstruct binary heap\n",
    "            self.priority_queue.rebalance(self.isFull)\n",
    "            if self.replace:\n",
    "                self.index = 1\n",
    "                return self.index\n",
    "            else:\n",
    "                sys.exit('Error: memory buffer is full and replace is set to FALSE!\\n')\n",
    "                return -1        \n",
    "        else:\n",
    "            self.index += 1\n",
    "            return self.index\n",
    "        \n",
    "    def store(self, experience):\n",
    "        '''\n",
    "        store experience for replay\n",
    "        arg: experience: <s, a, r, s', done>\n",
    "        '''\n",
    "        insert_index = self.get_index()\n",
    "        if insert_index > 0:\n",
    "            if insert_index in self.memory:\n",
    "                del self.memory[insert_index]\n",
    "            self.memory[insert_index] = experience\n",
    "            # add to priority queue\n",
    "            priority = self.priority_queue.get_max_priority()\n",
    "            self.priority_queue.update(priority, insert_index)\n",
    "        else:\n",
    "            sys.exit('Error: store failed!\\n')\n",
    "            return False\n",
    "\n",
    "    \n",
    "    def check_cross(self, segment): \n",
    "        '''\n",
    "        check which segment contains rank-id of previous segment;\n",
    "        rank-id has a probability big enough to include up to \"cross\" segment but not fully cover\n",
    "        arg: segment: segments as numpy array\n",
    "        return: cross: the cross segement as numpy array\n",
    "        '''\n",
    "        segment = list(segment)\n",
    "        cross = []\n",
    "        for i in range(len(segment)):\n",
    "            if i == (len(segment)-1):\n",
    "                break\n",
    "            elif segment[i] != segment[i+1]:\n",
    "                cross.append(int(segment[i] + 1))\n",
    "        cross = np.asarray(cross)\n",
    "        return cross\n",
    "\n",
    "    def build_distributions(self):\n",
    "        '''\n",
    "        pre-compute probability distribution of rank-based PER \n",
    "        '''\n",
    "        distributions = {}\n",
    "        partition_id = 1\n",
    "        partition_size = int (math.floor(self.memory_size / self.n_partitions))    \n",
    "        \n",
    "        for n in range(partition_size, self.memory_size + 1, partition_size):\n",
    "            distribution = {}         \n",
    "            # P(i) = (rank i) ^ (-alpha) / sum ((rank i) ^ (-alpha))\n",
    "            pmf = np.array([(1/i)**self.alpha for i in range(1, n + 1)]) \n",
    "            pmf_sum = pmf.sum()\n",
    "            pmf_norm = list(map(lambda x: x / pmf_sum, pmf)) \n",
    "            distribution['pmf'] = pmf_norm  \n",
    "            cdf = np.cumsum(pmf_norm)\n",
    "            boundary = cdf[-1]/self.k  \n",
    "            segment = cdf//boundary # which of the segment does each P(i) fall into\n",
    "            cross_segment = self.check_cross(segment)\n",
    "\n",
    "            ranges = [] # list of tuples (left,right) that are inclusive ranges of rank\n",
    "            prev_range = (1,1) \n",
    "            for i in range(self.k):\n",
    "                seg_point = np.nonzero(segment == (i+1))[0]\n",
    "                cross_point = np.nonzero(cross_segment == (i+1))[0]\n",
    "                if len(seg_point) > 0:\n",
    "                    if len(cross_point) > 0: #if the segment point is also where the cross point locates at\n",
    "                        this_range = (prev_range[-1], seg_point[-1]+1) #include the last object from previous range to this range \n",
    "                        ranges.append(this_range)\n",
    "                        prev_range = this_range  \n",
    "                    else:\n",
    "                        this_range = (seg_point[0]+1,seg_point[-1]+1)\n",
    "\n",
    "                        ranges.append(this_range)               \n",
    "                else: \n",
    "                    if len(cross_point) > 0: #if it is only a cross point, range is from previous one to the next one\n",
    "                        this_range = (prev_range[-1], prev_range[-1]+1)\n",
    "                        ranges.append(this_range)\n",
    "                        prev_range = (prev_range[-1]+1, prev_range[-1]+1) #replace the previous range with the next range index\n",
    "                    else:\n",
    "                        ranges.append(prev_range)\n",
    "            distribution[\"ranges\"] = ranges\n",
    "            distributions[partition_id] = distribution \n",
    "            partition_id += 1 \n",
    "        return distributions\n",
    "      \n",
    "               \n",
    "    def retrieve(self, e_id):\n",
    "        '''\n",
    "        get experience from memory by indexing experience id\n",
    "        return: list of memory tuples\n",
    "        '''\n",
    "        return [self.memory[i] for i in e_id]\n",
    "    \n",
    "    def sample(self, global_step): \n",
    "        '''\n",
    "        sample experience for prioritized memory reply\n",
    "        arg: global_step: current time step\n",
    "        return: experience id, experience, importance weights\n",
    "        '''\n",
    "        dist_index = int (math.floor(self.record_size / self.memory_size * self.n_partitions))\n",
    "        partition_size = int (math.floor(self.memory_size / self.n_partitions))\n",
    "        partition_max = dist_index * partition_size\n",
    "        distribution = self.distributions[dist_index]\n",
    "        rank_ids = []     \n",
    "        \n",
    "        for x in distribution[\"ranges\"]:\n",
    "            sampled_object = random.sample(range(x[0],x[-1]+1), 1) \n",
    "            rank_ids.append(sampled_object[0])\n",
    "        \n",
    "        # beta, increase by global_step, max 1\n",
    "        beta = min(self.beta + (global_step - self.train_start - 1) * self.beta_grad, 1)\n",
    "        alpha_pow = [distribution['pmf'][v - 1] for v in rank_ids]\n",
    "        # w = (P(i) * N) ^ (-beta) / max w\n",
    "        weights = np.power(np.array(alpha_pow) * partition_max, -beta)\n",
    "        weights = np.divide(weights, max(weights))\n",
    "        \n",
    "        e_id = self.priority_queue.get_experience_id(rank_ids)\n",
    "        experience = self.retrieve(e_id)\n",
    "        return (e_id, experience, weights)\n",
    "\n",
    "\n",
    "    def update_priority(self, e_id, td_error):\n",
    "        '''\n",
    "        update sampled experience priority with new td-error\n",
    "        arg: e_id: experience id\n",
    "        arg: td_error: corresponding td-error\n",
    "        '''\n",
    "        for i in range(0, len(e_id)):\n",
    "            self.priority_queue.update(abs(td_error[i]), e_id[i])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training / Testing the DQN agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    #initialize environment and the agent\n",
    "    env = gym.make('CartPole-v0')\n",
    "    state_size = env.observation_space.shape[0]\n",
    "    action_size = env.action_space.n\n",
    "    agent = DQNAgent(state_size, action_size)\n",
    "    done = False\n",
    "    batch_size = 32\n",
    "    rank = RankBuffer(batch_size)\n",
    "    #pre-compute distributions  \n",
    "    distributions=rank.build_distributions()\n",
    "   \n",
    "    episode_test = []\n",
    "    score_test = []\n",
    "\n",
    "    \n",
    "    #iterate through the game episode\n",
    "    for e in range(1000):\n",
    "        #reset state \n",
    "        state = env.reset()\n",
    "        state = np.reshape(state,[1,-1])    \n",
    "        for time_step in range(300):\n",
    "            action = agent.act(state)\n",
    "            #gather information from environment after taking current action\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            next_state = np.reshape(next_state, [1,-1])\n",
    "            reward_true = reward / 200\n",
    "            #put experience into the replay memory\n",
    "            experience = (state, action, reward_true, next_state, done)\n",
    "            rank.store(experience)\n",
    "                          \n",
    "            #make next state the current state\n",
    "            state = next_state\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "\n",
    "            if len(rank.memory) > agent.train_start:\n",
    "                e_id, experience, weights = rank.sample(time_step)\n",
    "                #print('sample experience:', e_id)\n",
    "                td_error = agent.replay(experience, weights)\n",
    "                #print('td_error for sample:', td_error)\n",
    "                #p = td_error.index(max(td_error))\n",
    "                #print('expereience has max td_error after update:', e_id[p])\n",
    "                rank.update_priority(e_id, td_error)\n",
    "                #print('priority queue after re-rank', rank.priority_queue.priority_queue)\n",
    "\n",
    "        \n",
    "        #update target model's weight to policy model's after every episode\n",
    "        if agent.mode == \"Double\":\n",
    "            agent.target_model.set_weights(agent.policy_model.get_weights())\n",
    "            \n",
    "    \n",
    "\n",
    "        #Evaluation Part: every 10 epsiodes, evaluate on agent's policy            \n",
    "        if e%10 == 0:\n",
    "            score = 0\n",
    "            episode_test.append(e)\n",
    "            for eval_e in range(50): #evaluate on 50 episodes's average rewards\n",
    "                state = env.reset()\n",
    "                state = np.reshape(state,[1,-1])\n",
    "                for eval_time_step in range(300):\n",
    "                    action = agent.act(state, False) \n",
    "                    next_state, reward, done, info = env.step(action)\n",
    "                    next_state = np.reshape(next_state, [1,-1])\n",
    "                    state = next_state\n",
    "                    if done:\n",
    "                        score += eval_time_step\n",
    "                        break\n",
    "            avg_score = score/50\n",
    "            score_test.append(avg_score)\n",
    "            print(\"trainning episode: {}/{}, avg score of 50 evaluation episodes: {}\".format(e, 1000, avg_score))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "381.85px",
    "left": "42px",
    "top": "194.57px",
    "width": "212px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
